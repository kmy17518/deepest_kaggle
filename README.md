# Question3 Kaggle

### Kaggleì— ì œì¶œí•œ submissionì˜ ìº¡ì³ì´ë¯¸ì§€

![submission](resource/submission_1.png)

> Cannot submit
Your Notebook cannot use internet access in this competition. Please disable internet in the Notebook editor and save a new version.
Your Notebook uses non-versioned datasets [/soulmachine/pretrained-bert-models-for-pytorch] (see Dataset Settings).

### í•™ìŠµ/ê²€ì¦ ë¡œê·¸

![train_loss](resource/train_1.png)
![val_loss](resource/val_1.png)

### ì†”ë£¨ì…˜ ì„¤ëª…

Knowledge Distillationì„ í™œìš©í•œ DistilBERTë¥¼ ëª¨ë¸ë¡œ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤.

DistilBERTë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•œ ë’¤ì—ëŠ” CLS Tokenì˜ Last Hidden Stateë¥¼ Sentence Embeddingìœ¼ë¡œ í™œìš©í–ˆìŠµë‹ˆë‹¤.

ì´í›„ Linear Layerë¥¼ í™œìš©í•´ì„œ Readabilityì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

### ì½”ë“œ ì¬í˜„ ë°©ë²•

dbert-train.ipynbë¥¼ ëŒë¦¬ë©´ state_listê°€ ë‚˜ì˜µë‹ˆë‹¤.

ì´ state_listì™€ ê³µìš© ë°ì´í„° distilbert-base-uncasedë¥¼ kaggleì— ì—…ë¡œë“œ í•œ ë’¤, dbert-inference.ipynbë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ì‹œë©´ ë©ë‹ˆë‹¤.

### ì°¸ê³ í•œ ìë£Œ

[[TRAINING & KFOLDS] PyTorch BERT-Large w/o OOMğŸ¯](https://www.kaggle.com/code/heyytanay/training-kfolds-pytorch-bert-large-w-o-oom)
